import torch
import numpy as np
import cv2
import os
from collections import deque
from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation

class SegFormerDetector:
    def __init__(
        self,
        model_name="nvidia/segformer-b2-finetuned-ade-512-512",
        device=None
    ):
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        
        # 1. åˆå§‹åŒ–æ¨¡å‹ä¸å¤„ç†å™¨
        self.processor = SegformerImageProcessor.from_pretrained(model_name)
        self.model = SegformerForSemanticSegmentation.from_pretrained(model_name).to(self.device)
        
        # [ä¼˜åŒ–] å¼€å¯åŠç²¾åº¦æ¨ç†ï¼ŒRTX 3090 æ€§èƒ½ç¿»å€
        if self.device == "cuda":
            self.model.half()
        self.model.eval()
        print(f"model classes: {self.get_labels()}")
        # ADE20K åœ°é¢ç±»åˆ«å®šä¹‰
        # æ ¸å¿ƒé€šè¡Œç±»ï¼šåœ°æ¿ã€é©¬è·¯ã€äººè¡Œé“ã€å°å¾„ã€åœŸåœ°ã€åœ°æ¯¯ã€åœŸåœ°
        self.ground_classes = [3, 6, 11, 13, 28, 52, 91, 94]
        # æ–°å¢ï¼šç”¨äºæ—¶åŸŸå¹³æ»‘çš„é˜Ÿåˆ—ï¼Œå­˜å‚¨æœ€è¿‘ 3 å¸§çš„ ground_mask
        self.mask_buffer = deque(maxlen=3)
        # ç»Ÿè®¡ç›¸å…³
        self.frame_counter = 0
        self.saved_images_count = 0

    def get_ground_contact_points(self, frame, render=True):
        self.frame_counter += 1
        
        # 1. æ¨¡å‹æ¨ç†è·å–å½“å‰å¸§åŸå§‹æ©ç  (Raw Mask)
        current_mask = self._inference(frame)

        # 2. [æ–°å¢] Temporal Smoothing: 3 å¸§ä¸­å€¼æ»¤æ³¢
        self.mask_buffer.append(current_mask)
        
        if len(self.mask_buffer) < 3:
            # ç¼“å†²åŒºæœªæ»¡æ—¶ï¼Œç›´æ¥ä½¿ç”¨å½“å‰å¸§
            smoothed_mask = current_mask
        else:
            # å°†é˜Ÿåˆ—ä¸­çš„ 3 ä¸ª mask å †å å¹¶å–ä¸­å€¼
            # å¯¹äºäºŒå€¼(0,1)æ©ç ï¼Œä¸­å€¼ç­‰åŒäºâ€œæŠ•ç¥¨åˆ¶â€ï¼š2å¸§ä»¥ä¸Šè®¤ä¸ºæ˜¯åœ°é¢ï¼Œç»“æœå°±æ˜¯åœ°é¢
            mask_stack = np.stack(self.mask_buffer, axis=0)
            smoothed_mask = np.median(mask_stack, axis=0).astype(np.uint8)

        # 3. ä½¿ç”¨å¹³æ»‘åçš„æ©ç æå–äº¤ç•Œç‚¹
        contact_pixels = self._extract_boundary_points(smoothed_mask)

        # 4. æ¸²æŸ“
        annotated_frame = None
        if render:
            # ä½¿ç”¨å¹³æ»‘åçš„ç»“æœè¿›è¡Œå¯è§†åŒ–
            annotated_frame = self._render_visualization(frame, smoothed_mask, contact_pixels)
            self.save_sample_image(annotated_frame)

        return contact_pixels, annotated_frame

    def _inference(self, frame):
        """[å†…éƒ¨å‡½æ•°] å¤„ç†æ¨¡å‹æ¨ç†"""
        # é¢œè‰²ç©ºé—´è½¬æ¢
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)        
        # é¢„å¤„ç†å¹¶è½¬ä¸ºåŠç²¾åº¦
        inputs = self.processor(images=img_rgb, return_tensors="pt").to(self.device)
        if self.device == "cuda":
            inputs = {k: v.to(dtype=torch.float16) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits

        # ä¸Šé‡‡æ ·å›åŸå›¾å°ºå¯¸
        upsampled_logits = torch.nn.functional.interpolate(
            logits, size=frame.shape[:2], mode='bilinear', align_corners=False
        )
        
        # è·å–åˆ†ç±»é¢„æµ‹å›¾
        pred_map = upsampled_logits.argmax(dim=1)[0].cpu().numpy()
        # self.print_detected_categories(pred_map)
        # ç”ŸæˆäºŒå€¼åŒ–çš„åœ°é¢æ©ç 
        return np.isin(pred_map, self.ground_classes).astype(np.uint8)

    def print_detected_categories(self, pred_map):
        """
        è¾“å…¥æ¨ç†å¾—åˆ°çš„ pred_map [H, W]
        æ‰“å°å½“å‰ç”»é¢ä¸­å‡ºç°çš„æ‰€æœ‰ç±»åˆ«åç§°
        """
        # 1. è·å–å›¾ä¸­å­˜åœ¨çš„æ‰€æœ‰å”¯ä¸€ ID
        unique_ids = np.unique(pred_map)
        
        # 2. è·å–æ˜ å°„è¡¨
        id2label = self.model.config.id2label
        
        print("\nğŸ” å½“å‰å¸§æ£€æµ‹åˆ°ä»¥ä¸‹ç±»å‹:")
        print("-" * 30)
        for cls_id in unique_ids:
            label = id2label.get(cls_id, f"Unknown({cls_id})")
            # ç»Ÿè®¡è¯¥ç±»åˆ«çš„åƒç´ å æ¯”ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¦ç‰¹å¾
            pixel_count = np.sum(pred_map == cls_id)
            percentage = (pixel_count / pred_map.size) * 100
            
            # æ ‡æ³¨è¯¥ç±»åˆ«æ˜¯å¦è¢«ä½ å½’ç±»ä¸ºâ€œåœ°é¢â€
            is_ground = " [åœ°é¢âœ…]" if cls_id in self.ground_classes else ""
            
            print(f"ID {cls_id:3} | {label:15} | å æ¯”: {percentage:5.2f}% {is_ground}")
              
    def get_labels(self):
        """è¿”å›æ‰€æœ‰ç±»åˆ«çš„å­—å…¸ {id: "label_name"}"""
        return self.model.config.id2label
    
    def _extract_boundary_points(self, ground_mask):
        contact_pixels = []
        h, w = ground_mask.shape
        step_x = 10

        for x in range(0, w, step_x):
            col = ground_mask[:, x]

            found = False
            # ä»åº•éƒ¨å‘ä¸Šæ‰«æ
            for y in range(h - 1, 0, -1):
                # åœ°é¢ â†’ éåœ°é¢ çš„è¾¹ç•Œ
                if col[y] == 1 and col[y - 1] == 0:
                    contact_pixels.append((float(x), float(y)))
                    found = True
                    break

            if not found:
                # æ•´åˆ—æ²¡æœ‰éšœç¢ï¼Œè®¤ä¸ºè§†é‡å¼€é˜”
                contact_pixels.append((float(x), float(0)))

        return contact_pixels

    def _render_visualization(self, frame, ground_mask, contact_pixels):
        """[å†…éƒ¨å‡½æ•°] ç»˜åˆ¶å¯è§†åŒ–æ•ˆæœ"""
        overlay = frame.copy()
        # ç»¿è‰²é«˜äº®åœ°é¢
        overlay[ground_mask == 1] = [0, 255, 0]
        canvas = cv2.addWeighted(overlay, 0.3, frame, 0.7, 0)

        # ç»˜åˆ¶çº¢è‰²çš„äº¤ç•Œç‚¹
        for pt in contact_pixels:
            cv2.circle(canvas, (int(pt[0]), int(pt[1])), 4, (0, 0, 255), -1)
            
        return canvas

    def save_sample_image(self, image, folder="samples", max_count=10, interval=20):
        """
        [ç‹¬ç«‹å‡½æ•°] å¤–éƒ¨è°ƒç”¨æ­¤å‡½æ•°æ¥å†³å®šæ˜¯å¦ä¿å­˜é‡‡æ ·å›¾ç‰‡
        """
        if self.saved_images_count >= max_count:
            return False
            
        if self.frame_counter % interval == 0:
            self.saved_images_count += 1
            os.makedirs(folder, exist_ok=True)
            path = os.path.join(folder, f"sample_{self.saved_images_count}.jpg")
            cv2.imwrite(path, image)
            print(f"ğŸ“¸ é‡‡æ ·ä¿å­˜æˆåŠŸ: {path} (Frame: {self.frame_counter})")
            return True
        return False

# =========================================================
# è¿è¡Œä¸»é€»è¾‘
# =========================================================
def main():
    detector = SegFormerDetector()
    
    test_image_path = "asset/test.png"
    frame = cv2.imread(test_image_path)
    
    if frame is not None:
        # 1. æ¨ç†ä¸ç‚¹æå–
        contact_pixels, visual_frame = detector.get_ground_contact_points(frame, render=True)
        
        # 2. ç‹¬ç«‹è°ƒç”¨ä¿å­˜é€»è¾‘
        detector.save_sample_image(visual_frame, max_count=5, interval=1)
        
        print(f"ğŸ”¹ æ£€æµ‹åˆ° {len(contact_pixels)} ä¸ªæ¥è§¦ç‚¹")
        # cv2.imshow("Optimized SegFormer", visual_frame)
        # cv2.waitKey(0)

if __name__ == "__main__":
    main()