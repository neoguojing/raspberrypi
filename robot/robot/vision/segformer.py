import torch
import numpy as np
import cv2
import os
from collections import deque
import time
from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation

class SegFormerDetector:
    def __init__(
        self,
        model_name="nvidia/segformer-b2-finetuned-ade-512-512",
        device=None,
        alpha=0.6,          # æ—¶é—´åŸŸå¹³æ»‘ç³»æ•°ï¼Œè¶Šå¤§è¶Šå¹³æ»‘(å»¶è¿Ÿè¶Šé«˜)
        conf_threshold=0.3  # ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼ä¸è®¤ä¸ºæ˜¯åœ°é¢
    ):
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        
        # 1. åˆå§‹åŒ–æ¨¡å‹ä¸å¤„ç†å™¨
        self.processor = SegformerImageProcessor.from_pretrained(model_name)
        self.model = SegformerForSemanticSegmentation.from_pretrained(model_name).to(self.device)
        
        if self.device == "cuda":
            self.model.half()
        self.model.eval()

        # ADE20K åœ°é¢ç±»åˆ«å®šä¹‰
        self.ground_classes = [3, 6, 11, 13, 28, 52, 91, 94, 21, 9, 60, 46, 52, 54, 26, 109, 27, 147, 131]
        
        # ç¨³å®šæ€§å¢å¼ºç›¸å…³å‚æ•°
        self.alpha = alpha
        self.conf_threshold = conf_threshold
        self.ema_probs = None  # å­˜å‚¨æ¦‚ç‡å›¾çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡
        self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        
        self.frame_counter = 0
        self.saved_images_count = 0
        self.last_save_time = 0

    def _preprocess_lighting(self, frame):
        """ä½¿ç”¨ CLAHE å¢å¼ºå¯¹æ¯”åº¦ï¼ŒæŠ‘åˆ¶å¼ºå…‰å’Œæš—é˜´å½±çš„å½±å“"""
        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        cl = self.clahe.apply(l)
        limg = cv2.merge((cl, a, b))
        return cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)

    def _inference(self, frame):
        """æ ¸å¿ƒæ¨ç†é€»è¾‘ï¼šåŒ…å«æ¦‚ç‡å¹³æ»‘å’Œç½®ä¿¡åº¦è¿‡æ»¤"""
        # 1. é¢„å¤„ç†
        img_rgb = self._preprocess_lighting(frame)
        inputs = self.processor(images=img_rgb, return_tensors="pt").to(self.device)
        
        if self.device == "cuda":
            inputs = {k: v.to(dtype=torch.float16) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
            # è·å–æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ (Softmax)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)

        # 2. ä¸Šé‡‡æ ·åˆ°åŸå›¾å°ºå¯¸ (B, C, H, W)
        upsampled_probs = torch.nn.functional.interpolate(
            probs, size=frame.shape[:2], mode='bilinear', align_corners=False
        )[0] # å–å‡ºç¬¬ä¸€å¼ å›¾
        
        current_probs = upsampled_probs.cpu().numpy()

        # 3. æ—¶é—´åŸŸå¹³æ»‘ (EMA) - åœ¨æ¦‚ç‡å±‚é¢ä¸Šå¹³æ»‘æ¯”åœ¨ Label å±‚é¢æ›´ç¨³
        if self.ema_probs is None:
            self.ema_probs = current_probs
        else:
            self.ema_probs = self.alpha * self.ema_probs + (1 - self.alpha) * current_probs

        # 4. è·å–å½“å‰æœ€é«˜æ¦‚ç‡åŠå…¶ç´¢å¼•
        max_conf = np.max(self.ema_probs, axis=0)
        pred_map = np.argmax(self.ema_probs, axis=0)

        # 5. ç½®ä¿¡åº¦è¿‡æ»¤ï¼šå¦‚æœæ¨¡å‹å¯¹è‡ªå·±æ²¡ä¿¡å¿ƒï¼Œå°±åˆ¤å®šä¸ºéåœ°é¢
        # ä¸” åªæœ‰åœ¨ ground_classes ä¸­çš„æ‰åˆ¤å®šä¸º 1
        ground_mask = np.isin(pred_map, self.ground_classes)
        ground_mask = np.where((ground_mask) & (max_conf > self.conf_threshold), 1, 0).astype(np.uint8)

        # 6. å½¢æ€å­¦åå¤„ç†ï¼šå»é™¤ç»†å°å™ªç‚¹ï¼Œå¡«å……é˜´å½±ç©ºæ´
        kernel = np.ones((5, 5), np.uint8)
        ground_mask = cv2.morphologyEx(ground_mask, cv2.MORPH_CLOSE, kernel) # å¡«æ´
        ground_mask = cv2.morphologyEx(ground_mask, cv2.MORPH_OPEN, kernel)  # å»å™ª

        return ground_mask

    def _extract_boundary_points_optimized(self, ground_mask, step_x=10):
        """é«˜æ•ˆæå–åœ°é¢è¾¹ç¼˜ç‚¹"""
        h, w = ground_mask.shape
        sampled_mask = ground_mask[:, ::step_x]
        
        # å¯»æ‰¾ 1 -> 0 çš„è·³å˜ (ä»ä¸‹å¾€ä¸Šæ‰«æçš„é€»è¾‘ç®€åŒ–ç‰ˆ)
        diff = sampled_mask[:-1, :] - sampled_mask[1:, :]
        y_coords, x_idx = np.where(diff == 1)
        
        contact_dict = {}
        for x, y in zip(x_idx, y_coords):
            real_x = x * step_x
            # ä¿ç•™æ¯åˆ—æœ€é ä¸‹çš„è¾¹ç•Œç‚¹
            if real_x not in contact_dict or y > contact_dict[real_x]:
                contact_dict[real_x] = y

        contact_pixels = []
        for i, x in enumerate(range(0, w, step_x)):
            if x in contact_dict:
                contact_pixels.append((float(x), float(contact_dict[x])))
            else:
                # åº•éƒ¨æ˜¯åœ°é¢åˆ™çœ‹ä½œè¿œå¤„(0)ï¼Œå¦åˆ™çœ‹ä½œè„šä¸‹(h-1)
                y_val = 0.0 if sampled_mask[-1, i] == 1 else float(h - 1)
                contact_pixels.append((float(x), y_val))

        return contact_pixels

    def get_ground_contact_points(self, frame, render=True):
        self.frame_counter += 1
        
        # æ‰§è¡Œå¸¦æœ‰ç¨³å®šæ€§ä¼˜åŒ–çš„æ¨ç†
        smoothed_mask = self._inference(frame)
        
        # æå–äº¤ç•Œç‚¹
        contact_pixels = self._extract_boundary_points_optimized(smoothed_mask)

        annotated_frame = None
        if render:
            annotated_frame = self._render_visualization(frame, smoothed_mask, contact_pixels)

        return contact_pixels, annotated_frame

    def _render_visualization(self, frame, ground_mask, contact_pixels):
        overlay = frame.copy()
        overlay[ground_mask == 1] = [0, 255, 0] # ç»¿è‰²é«˜äº®åœ°é¢
        canvas = cv2.addWeighted(overlay, 0.3, frame, 0.7, 0)

        for pt in contact_pixels:
            cv2.circle(canvas, (int(pt[0]), int(pt[1])), 4, (0, 0, 255), -1)
        return canvas

    def save_sample_image(self, visual_frame, folder="samples", max_count=10, interval=5):
        if visual_frame is None: return
        
        curr_time = time.time()
        if curr_time - self.last_save_time >= interval:
            self.last_save_time = curr_time
            save_index = (self.saved_images_count % max_count) + 1
            self.saved_images_count += 1
            
            os.makedirs(folder, exist_ok=True)
            path = os.path.join(folder, f"sample_{save_index}.jpg")
            cv2.imwrite(path, visual_frame)
            print(f"ğŸ“¸ ç¨³å®šé‡‡æ ·ä¿å­˜: {path}")

# =========================================================
# è¿è¡Œä¸»é€»è¾‘ (æ¨¡æ‹Ÿè§†é¢‘æµ)
# =========================================================
def main():
    # è°ƒé«˜ alpha å¯ä»¥è®©é¢„æµ‹æ›´è¿Ÿé’ä½†æ›´ç¨³ï¼ˆé€‚åˆå…‰ç…§å‰§çƒˆå˜åŒ–åœºæ™¯ï¼‰
    detector = SegFormerDetector(alpha=0.7, conf_threshold=0.4)
    
    # æ¨¡æ‹Ÿå¤„ç†ï¼šè¿™é‡Œå‡è®¾ä½ æœ‰ä¸€ä¸ªè§†é¢‘æ–‡ä»¶æˆ–æ‘„åƒå¤´
    # cap = cv2.VideoCapture("video.mp4")
    test_image_path = "asset/test.png"
    frame = cv2.imread(test_image_path)

    if frame is not None:
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¾ªç¯è°ƒç”¨ get_ground_contact_points
        for i in range(5): # æ¨¡æ‹Ÿå¤šå¸§è¾“å…¥ä»¥è§¦å‘ EMA å¹³æ»‘
            contact_pixels, visual_frame = detector.get_ground_contact_points(frame, render=True)
            detector.save_sample_image(visual_frame, interval=0) # å¼ºåˆ¶ä¿å­˜æµ‹è¯•
            
        print(f"ğŸ”¹ æ£€æµ‹å®Œæˆï¼Œå½“å‰å¸§æ¥è§¦ç‚¹æ•°é‡: {len(contact_pixels)}")

if __name__ == "__main__":
    main()