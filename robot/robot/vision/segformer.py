import torch
import numpy as np
import cv2
import os
import time
from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation

class SegFormerDetector:
    def __init__(
        self,
        model_name="nvidia/segformer-b2-finetuned-ade-512-512",
        device=None,
        alpha=0.7,              # EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰ç³»æ•°ã€‚å€¼è¶Šå¤§ï¼Œå¯¹å†å²å¸§ä¾èµ–è¶Šå¼ºï¼Œç”»é¢è¶Šç¨³ï¼Œä½†åŠ¨æ€å“åº”è¶Šæ…¢
        conf_threshold=0.25     # åœ°é¢ç½®ä¿¡åº¦é˜ˆå€¼ã€‚è¿‡æ»¤æ‰æ¨¡å‹çŠ¹è±«ä¸å†³çš„é¢„æµ‹åŒºåŸŸ
    ):
        # è‡ªåŠ¨é€‰æ‹©ç¡¬ä»¶ï¼šä¼˜å…ˆä½¿ç”¨ CUDA
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # åŠ è½½ SegFormer å¤„ç†å™¨å’Œé¢„è®­ç»ƒæ¨¡å‹
        self.processor = SegformerImageProcessor.from_pretrained(model_name)
        self.model = SegformerForSemanticSegmentation.from_pretrained(model_name).to(self.device)

        # å¦‚æœä½¿ç”¨ GPUï¼Œå¼€å¯åŠç²¾åº¦æ¨ç† (FP16)ï¼Œå¯æ˜¾è‘—æå‡ RTX æ˜¾å¡çš„æ¨ç†é€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜å ç”¨
        if self.device == "cuda":
            self.model.half()
        self.model.eval()

        # ADE20K åè®®ä¸­å®šä¹‰çš„åœ°é¢ã€è·¯é¢ç›¸å…³ç±»åˆ« ID
        # èšåˆè¿™äº› ID å¯ä»¥å¿½ç•¥å…·ä½“çš„è·¯é¢æè´¨ï¼ˆåœ°æ¯¯ã€è‰åœ°æˆ–æŸæ²¹è·¯ï¼‰ï¼Œç»Ÿä¸€è§†ä¸ºâ€œå¯è¡Œèµ°åŒºåŸŸâ€
        self.ground_classes = [
            3, 6, 11, 13, 21, 26, 27, 28,
            46, 52, 54, 60, 91, 94, 109, 131, 147
        ]

        self.alpha = alpha
        self.conf_threshold = conf_threshold
        self.ema_ground_prob = None  # ç”¨äºç¼“å­˜ä¸Šä¸€å¸§å¹³æ»‘åçš„æ¦‚ç‡å›¾

        # åˆå§‹åŒ– CLAHE (å¯¹æ¯”åº¦å—é™çš„è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–)
        # ç”¨äºæŠ‘åˆ¶å›¾åƒä¸­çš„å¼ºå…‰é—ªçƒï¼Œå¹¶å¢å¼ºæš—éƒ¨é˜´å½±ä¸­çš„ç»†èŠ‚çº¹ç†
        self.clahe = cv2.createCLAHE(2.0, (8, 8))

    # ----------------------------------------------------
    # å›¾åƒé¢„å¤„ç†ï¼šå…‰ç…§å¹³è¡¡
    # ----------------------------------------------------
    def _preprocess_lighting(self, frame):
        """
        é€šè¿‡ LAB è‰²å½©ç©ºé—´å¯¹äº®åº¦é€šé“(L)è¿›è¡Œå‡è¡¡åŒ–ï¼Œ
        è§£å†³å®¤å†…å¤–å…‰ç…§ä¸å‡æˆ–é˜´å½±å¯¼è‡´çš„è·¯é¢æ¼æ£€é—®é¢˜ã€‚
        """
        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        l = self.clahe.apply(l)  # ä»…å¢å¼ºäº®åº¦ï¼Œä¸ç ´åè‰²å½©å¹³è¡¡
        lab = cv2.merge((l, a, b))
        return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)

    def print_detected_categories(self, pred_map):
        """
        è¾“å…¥æ¨ç†å¾—åˆ°çš„ pred_map [H, W]
        æ‰“å°å½“å‰ç”»é¢ä¸­å‡ºç°çš„æ‰€æœ‰ç±»åˆ«åç§°
        """
        # 1. è·å–å›¾ä¸­å­˜åœ¨çš„æ‰€æœ‰å”¯ä¸€ ID
        unique_ids = np.unique(pred_map)
        
        # 2. è·å–æ˜ å°„è¡¨
        id2label = self.model.config.id2label
        
        print("\nğŸ” å½“å‰å¸§æ£€æµ‹åˆ°ä»¥ä¸‹ç±»å‹:")
        print("-" * 30)
        for cls_id in unique_ids:
            label = id2label.get(cls_id, f"Unknown({cls_id})")
            # ç»Ÿè®¡è¯¥ç±»åˆ«çš„åƒç´ å æ¯”ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¦ç‰¹å¾
            pixel_count = np.sum(pred_map == cls_id)
            percentage = (pixel_count / pred_map.size) * 100
            
            # æ ‡æ³¨è¯¥ç±»åˆ«æ˜¯å¦è¢«ä½ å½’ç±»ä¸ºâ€œåœ°é¢â€
            is_ground = " [åœ°é¢âœ…]" if cls_id in self.ground_classes else ""
            
            print(f"ID {cls_id:3} | {label:15} | å æ¯”: {percentage:5.2f}% {is_ground}")
    
    def print_detected_categories(self, pred_map):
        """
        è¾“å…¥æ¨ç†å¾—åˆ°çš„ pred_map [H, W]
        æ‰“å°å½“å‰ç”»é¢ä¸­å‡ºç°çš„æ‰€æœ‰ç±»åˆ«åç§°
        """
        # 1. è·å–å›¾ä¸­å­˜åœ¨çš„æ‰€æœ‰å”¯ä¸€ ID
        unique_ids = np.unique(pred_map)
        
        # 2. è·å–æ˜ å°„è¡¨
        id2label = self.model.config.id2label
        
        print("\nğŸ” å½“å‰å¸§æ£€æµ‹åˆ°ä»¥ä¸‹ç±»å‹:")
        print("-" * 30)
        for cls_id in unique_ids:
            label = id2label.get(cls_id, f"Unknown({cls_id})")
            # ç»Ÿè®¡è¯¥ç±»åˆ«çš„åƒç´ å æ¯”ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¦ç‰¹å¾
            pixel_count = np.sum(pred_map == cls_id)
            percentage = (pixel_count / pred_map.size) * 100
            
            # æ ‡æ³¨è¯¥ç±»åˆ«æ˜¯å¦è¢«ä½ å½’ç±»ä¸ºâ€œåœ°é¢â€
            is_ground = " [åœ°é¢âœ…]" if cls_id in self.ground_classes else ""
            
            print(f"ID {cls_id:3} | {label:15} | å æ¯”: {percentage:5.2f}% {is_ground}")

    # ----------------------------------------------------
    # æ ¸å¿ƒæ¨ç†é€»è¾‘ï¼šæ—¶åŸŸå¹³æ»‘ + æ¦‚ç‡è¿‡æ»¤
    # ----------------------------------------------------
    def _inference(self, frame):
        # 1. å¢å¼ºå›¾åƒå¹¶å‡†å¤‡æ¨¡å‹è¾“å…¥
        img = self._preprocess_lighting(frame)
        inputs = self.processor(images=img, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        if self.device == "cuda":
            inputs = {k: v.half() for k, v in inputs.items()}

        with torch.no_grad():
            logits = self.model(**inputs).logits
            # å°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ (Softmax)
            probs = torch.softmax(logits, dim=1)

        # 2. å°†ä½åˆ†è¾¨ç‡çš„ Logits åŒçº¿æ€§æ’å€¼å›åŸå§‹å›¾åƒå°ºå¯¸
        probs = torch.nn.functional.interpolate(
            probs,
            size=frame.shape[:2],
            mode="bilinear",
            align_corners=False
        )[0].cpu().numpy()

        pred_map = probs.argmax(dim=1)[0].cpu().numpy()
        self.print_detected_categories(pred_map)
        # 3. æå–åœ°é¢ç›¸å…³ç±»åˆ«çš„æœ€å¤§æ¦‚ç‡å€¼
        # ç»“æœæ˜¯ä¸€ä¸ª [H, W] çš„çŸ©é˜µï¼Œæ¯ä¸ªåƒç´ å€¼ä»£è¡¨â€œè¯¥ç‚¹å±äºåœ°é¢â€çš„ä¿¡å¿ƒå¾—åˆ†
        ground_prob = probs[self.ground_classes].max(axis=0)

        # 4. æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA) å¹³æ»‘å¤„ç†
        # ä½œç”¨ï¼šè¿‡æ»¤æ‰ç”±äºå•å¸§å™ªç‚¹ã€åŠ¨æ€æ¨¡ç³Šæˆ–å¿«é€Ÿé˜´å½±æ¼‚ç§»å¼•èµ·çš„â€œæ£€æµ‹ç©ºæ´â€
        if self.ema_ground_prob is None:
            self.ema_ground_prob = ground_prob
        else:
            # è¿™é‡Œçš„å¹³æ»‘å‘ç”Ÿåœ¨æ¦‚ç‡ç©ºé—´ï¼Œæ¯”åœ¨ Mask (0/1) ç©ºé—´å¹³æ»‘æ›´åŠ ç»†è…»
            self.ema_ground_prob = (
                self.alpha * self.ema_ground_prob
                + (1 - self.alpha) * ground_prob
            )

        # 5. äºŒå€¼åŒ–ï¼šåªæœ‰å½“å¹³æ»‘åçš„åœ°é¢æ¦‚ç‡è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œæ‰åˆ¤å®šä¸ºåœ°é¢
        ground_mask = (self.ema_ground_prob > self.conf_threshold).astype(np.uint8)

        # 6. å½¢æ€å­¦é—­è¿ç®— (Closing)
        # ä½œç”¨ï¼šå¡«å……åœ°é¢æ©ç ä¸­ç»†å°çš„é»‘è‰²ç©ºæ´ï¼ˆå¦‚åœ°ç –ç¼éš™ã€ç»†å°é˜´å½±ï¼‰ï¼ŒåŒæ—¶ä¿æŒè¾¹ç¼˜ä½ç½®å‡†ç¡®
        kernel = np.ones((3, 3), np.uint8)
        ground_mask = cv2.morphologyEx(ground_mask, cv2.MORPH_CLOSE, kernel)

        return ground_mask

    # ----------------------------------------------------
    # è¾¹ç•Œæå–ï¼šå‘é‡åŒ–æ‰«æ
    # ----------------------------------------------------
    def _extract_boundary_points(self, ground_mask, step_x=10):
        """
        ä»åº•éƒ¨å‘ä¸Šæ‰«æï¼Œå¯»æ‰¾åœ°é¢ä¸éåœ°é¢ï¼ˆéšœç¢ç‰©ï¼‰çš„äº¤ç•Œçº¿ç‚¹ã€‚
        ä½¿ç”¨å‘é‡åŒ–æ“ä½œä»£æ›¿ Python å¾ªç¯ï¼Œæå¤§æå‡æ•ˆç‡ã€‚
        """
        h, w = ground_mask.shape
        # æŒ‰æ­¥é•¿é‡‡æ ·åˆ—ï¼Œå‡å°‘è®¡ç®—é‡
        sampled = ground_mask[:, ::step_x]

        # è®¡ç®—å‚ç›´æ–¹å‘å·®åˆ†ã€‚å½“ç»“æœä¸º 1 æ—¶ï¼Œä»£è¡¨å‘ç”Ÿäº† 1(åœ°é¢) -> 0(éšœç¢ç‰©) çš„è·³å˜
        # 
        diff = sampled[:-1] - sampled[1:]
        ys, xs = np.where(diff == 1)

        # é¢„è®¾æ‰€æœ‰åˆ—çš„äº¤ç•Œç‚¹éƒ½åœ¨å›¾åƒæœ€åº•éƒ¨ (h-1)
        bottom_y = np.full(sampled.shape[1], h - 1)

        # å¯¹äºæ¯ä¸€åˆ—ï¼Œè®°å½•æœ€é è¿‘å›¾åƒä¸‹æ–¹çš„è·³å˜ç‚¹ï¼ˆå³æœ€è¿‘çš„éšœç¢ç‰©æ¥è§¦ç‚¹ï¼‰
        for y, x in zip(ys, xs):
            # å› ä¸ºæ‰«ææ˜¯ä»ä¸Šå¾€ä¸‹çš„ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°è¯¥åˆ—æœ€å¤§çš„ yï¼ˆæœ€é ä¸‹ï¼‰
            # è¿™é‡Œçš„é€»è¾‘é€šè¿‡éå†æ›´æ–°ï¼Œä¿è¯ bottom_y å­˜å‚¨çš„æ˜¯æœ€é ä¸‹çš„è¾¹ç•Œç‚¹
            bottom_y[x] = min(bottom_y[x], y)

        # è¿˜åŸå›åŸå§‹å›¾åƒçš„ x åæ ‡å¹¶æ‰“åŒ…æˆåæ ‡å¯¹
        contact_pixels = [
            (float(x * step_x), float(y))
            for x, y in enumerate(bottom_y)
        ]
        return contact_pixels

    # ----------------------------------------------------
    # å¯è§†åŒ–æ¸²æŸ“
    # ----------------------------------------------------
    def render(self, frame, mask, points):
        """
        åœ¨åŸå›¾ä¸Šå åŠ ç»¿è‰²é€æ˜åœ°é¢è’™ç‰ˆå’Œçº¢è‰²è¾¹ç¼˜è§¦ç‚¹ã€‚
        """
        vis = frame.copy()
        vis[mask == 1] = [0, 255, 0]  # åœ°é¢æ¶‚ç»¿
        # æ··åˆåŸå›¾ä¸è’™ç‰ˆ
        vis = cv2.addWeighted(vis, 0.3, frame, 0.7, 0)

        # ç»˜åˆ¶è¾¹ç•Œæ¥è§¦ç‚¹
        for x, y in points:
            cv2.circle(vis, (int(x), int(y)), 4, (0, 0, 255), -1)
        return vis

    # ----------------------------------------------------
    # å¤–éƒ¨ç»Ÿä¸€æ¥å£
    # ----------------------------------------------------
    def get_ground_contact_points(self, frame, render=True):
        """
        è¾“å…¥ BGR å›¾åƒï¼Œè¿”å›è¾¹ç•Œç‚¹åˆ—è¡¨åŠï¼ˆå¯é€‰çš„ï¼‰å¯è§†åŒ–ç»“æœã€‚
        """
        mask = self._inference(frame)
        points = self._extract_boundary_points(mask)
        vis = self.save_sample_image(frame, mask, points) if render else None
        return points, vis

    # ----------------------------------------------------
    # [è¡¥å……] è‡ªåŠ¨é‡‡æ ·ä¿å­˜æ¥å£
    # ----------------------------------------------------
    def save_sample_image(self, frame, mask, points, folder="samples", max_count=10, interval_seconds=10):
        """
        æŒ‰æ—¶é—´é—´éš”è‡ªåŠ¨ä¿å­˜æ£€æµ‹ç»“æœå›¾ï¼Œç”¨äºç¦»çº¿åˆ†æç¨³å®šæ€§ã€‚
        
        Args:
            visual_frame: render() æ–¹æ³•è¿”å›çš„å¯è§†åŒ– BGR å›¾åƒ
            folder: ä¿å­˜æ–‡ä»¶å¤¹è·¯å¾„
            max_count: æœ€å¤§ä¿å­˜æ•°é‡ï¼Œè¾¾åˆ°åä¼šå¾ªç¯è¦†ç›–ï¼ˆæ»šåŠ¨è®°å½•ï¼‰
            interval_seconds: ä¿å­˜çš„æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…é¢‘ç¹å†™ç£ç›˜
        """

        # åˆå§‹åŒ–è®¡æ•°å™¨å’Œæ—¶é—´è®°å½•ï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ‰§è¡Œï¼‰
        if not hasattr(self, 'last_save_time'):
            self.last_save_time = 0
        if not hasattr(self, 'saved_images_count'):
            self.saved_images_count = 0

        current_time = time.time()

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä¿å­˜çš„æ—¶é—´é—´éš”
        if current_time - self.last_save_time >= interval_seconds:
            visual_frame = self.render(frame, mask, points)
            # æ›´æ–°æœ€åä¿å­˜æ—¶é—´
            self.last_save_time = current_time
            
            # è®¡ç®—æ»šåŠ¨ç´¢å¼• (ä¾‹å¦‚ 1, 2, 3...10, 1, 2...)
            save_index = (self.saved_images_count % max_count) + 1
            self.saved_images_count += 1
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(folder, exist_ok=True)
            
            # æ–‡ä»¶åæ‹¼æ¥ï¼šåŒ…å«ç´¢å¼•ä»¥å®ç°è‡ªåŠ¨å¾ªç¯è¦†ç›–
            file_path = os.path.join(folder, f"sample_{save_index}.jpg")
            
            # æ‰§è¡Œå†™å…¥
            cv2.imwrite(file_path, visual_frame)
            
            print(f"ğŸ“¸ [é‡‡æ ·æˆåŠŸ] å·²ä¿å­˜è‡³: {file_path} (ç´¯è®¡ä¿å­˜: {self.saved_images_count} å¼ )")

            return visual_frame
        
        return None
# ========================================================
# æµ‹è¯•å…¥å£
# ========================================================
def main():
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = SegFormerDetector(alpha=0.7, conf_threshold=0.25)

    frame = cv2.imread("asset/test.png")
    if frame is None:
        print("âŒ é”™è¯¯ï¼šæ— æ³•åŠ è½½æµ‹è¯•å›¾ç‰‡ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    # æ¨¡æ‹Ÿè§†é¢‘æµå¤„ç†è¿‡ç¨‹ï¼Œè§‚å¯Ÿ EMA å¹³æ»‘æ•ˆæœ
    print("ğŸš€ æ­£åœ¨æ¨¡æ‹Ÿå¤„ç† 5 å¸§è¿ç»­å›¾åƒ...")
    for _ in range(5):
        pts, vis = detector.get_ground_contact_points(frame)

    print(f"âœ… æˆåŠŸï¼å½“å‰å¸§æ£€æµ‹åˆ° {len(pts)} ä¸ªè¾¹ç•Œå¼•å¯¼ç‚¹ã€‚")

if __name__ == "__main__":
    main()